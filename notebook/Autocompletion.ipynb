{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Autocompletion.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrzsCB_aAr25"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnYUZhZ6A7dT"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oomV532GA_Km"
      },
      "source": [
        "FILENAME = \"data/en_US.twitter.txt\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsetol5RBJSp"
      },
      "source": [
        "def load_data(filename):\n",
        "    with open(filename, \"rb\") as data:\n",
        "        text = data.read()\n",
        "    text = text.decode(\"utf-8\")\n",
        "    return text"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amhEyWxoBP9e"
      },
      "source": [
        "data = load_data(FILENAME)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f77b6T8ICBgX"
      },
      "source": [
        "def tokenize(data, return_sentences = True):\n",
        "    sentences = data.split('\\r\\n')\n",
        "    sentences = [sent.strip() for sent in sentences]\n",
        "    sentences = [sent for sent in sentences if len(sent) > 0]\n",
        "\n",
        "    word_tokens = [word_tokenize(sent.lower()) for sent in sentences]\n",
        "    if return_sentences:\n",
        "        return sentences, word_tokens\n",
        "    return word_tokens"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2Bd1CdjDtQk"
      },
      "source": [
        "sentences, word_tokens = tokenize(data)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwnH_EHpD-Cr"
      },
      "source": [
        "def split_data(word_tokens, test_size=0.2, random_state=42):\n",
        "    train_data, test_data = train_test_split(word_tokens, test_size=0.2, random_state=42)\n",
        "    return train_data, test_data"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9aOdUsWFtHZ"
      },
      "source": [
        "train_data, test_data = split_data(word_tokens)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QHu8xtAF0M4"
      },
      "source": [
        "### Count Words and Select vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUJXmUNyFxT4"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "def count_words(data):\n",
        "    cnt = Counter()\n",
        "    for words in data:\n",
        "        cnt.update(words)\n",
        "    return cnt"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJcbd54mGFNs"
      },
      "source": [
        "word_count = count_words(train_data)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wqHRsc6Inb4"
      },
      "source": [
        "# [word[0] for word in word_count.most_common(10)]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-e5bPsKGnbN"
      },
      "source": [
        "def create_vocabulary(word_count, min_frequency=0, max_words=0):\n",
        "    if min_frequency > 0:\n",
        "        vocabulary = []\n",
        "        for word, count in word_count.items():\n",
        "            if count >= min_frequency:\n",
        "                vocabulary.append(word)\n",
        "    if max_words > 0:\n",
        "        vocabulary = [word[0] for word in word_count.most_common(max_words)]\n",
        "\n",
        "    return vocabulary"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPX21wv3IAvb"
      },
      "source": [
        "# word_count.items()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQvMebbjKAgF"
      },
      "source": [
        "### Replace words by unknown tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CH1z9Il8JXY4"
      },
      "source": [
        "def handle_unk_words(data, vocabulary, unk_token=\"<unk>\"):\n",
        "    handled_data = []\n",
        "    for sentences in data:\n",
        "        sentence = []\n",
        "        for word in sentences:\n",
        "            if word in vocabulary:\n",
        "                sentence.append(word)\n",
        "            else:\n",
        "                sentence.append(unk_token)\n",
        "        handled_data.append(sentence)\n",
        "    return handled_data"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwWHNKrYKTR1"
      },
      "source": [
        "vocabulary = create_vocabulary(word_count, min_frequency=3)\n",
        "preprocessed_data = handle_unk_words(train_data, vocabulary=vocabulary)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48XAU8QIKXo8"
      },
      "source": [
        "preprocessed_data_test = handle_unk_words(test_data, vocabulary=vocabulary)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7Yj3qzBMIEH"
      },
      "source": [
        "## Create n-gram model\n",
        "### Count n-grams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1sHBTYxLZsj"
      },
      "source": [
        "from collections import defaultdict\n",
        "def count_n_grams(data, n, start_token=\"<s>\", end_token=\"<e>\"):\n",
        "    \"\"\"\n",
        "    Define and count the numbers of n-grams in text corpus.\n",
        "\n",
        "    Args:\n",
        "        data : preprocessed text corpus\n",
        "        n : n in n_gram\n",
        "        start_token: starting token symbol\n",
        "        end_token: ending token symbol\n",
        "\n",
        "    Returns:\n",
        "        n_grams: Dictionary containing n_gram tuples as keys and their counts as\n",
        "        values\n",
        "    \"\"\"\n",
        "\n",
        "    n_grams = defaultdict(int)\n",
        "    for sentence in data:\n",
        "        sentence = [start_token]*n + sentence + [end_token]\n",
        "        \n",
        "        \"\"\"\n",
        "            If we are calculating 1-gram we need to include all single words so we\n",
        "            need to loop over all the words of sentence\n",
        "            but if n > 2 we need to include last word only once, so we cannot\n",
        "            loop over all the words but n-1\n",
        "            e.g. ```\n",
        "                A = [\"A\",\"B\",\"C\",\"D\"]\n",
        "                if n=1, then len = len(A) = 4,\n",
        "                A1 = [\"A\"], A2 = [\"B\"], A3=[\"C\"], A4=[\"D\"]\n",
        "                if n> 2,\n",
        "                then len = len(A)-1 = 3\n",
        "                A1 = [\"A\",\"B\"], A2 = [\"B\",\"C\"] A3 = [\"C\",\"D\"]\n",
        "            ```\n",
        "        \"\"\"\n",
        "        m = len(sentence) if n == 1 else len(sentence)-1\n",
        "\n",
        "\n",
        "        for i in range(m):\n",
        "            n_gram = sentence[i:i+n]\n",
        "            n_gram = tuple(n_gram)\n",
        "            n_grams[n_gram] += 1\n",
        "    return n_grams"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RovnXOyRk0N"
      },
      "source": [
        "def calculate_probability(word, previous_n_gram, \n",
        "                          n_gram_counts, n_plusone_gram_counts,\n",
        "                          vocabulary_size,\n",
        "                          k=1):\n",
        "    \n",
        "    \"\"\"\n",
        "    Calculates the probability of given word in sentences given the previous n\n",
        "    words (n-gram)\n",
        "\n",
        "    Args:\n",
        "        word: current word\n",
        "        previous_n_gram: Previous n words\n",
        "        n_gram_counts : dictionary of n-gram counts\n",
        "        n_plusone_gram_counts: dictionary of (n+1)-gram counts\n",
        "        vocabulary_size: Size of the vocabulary\n",
        "        k: smoothing constant\n",
        "\n",
        "    Returns:\n",
        "        Probability\n",
        "        P = {count(previous_n_gram + current_word) + k} \n",
        "            / {count(previous_n_gram)+ k * voc_size}\n",
        "    \"\"\"\n",
        "\n",
        "    n_plus_one_gram = tuple(previous_n_gram + [word])\n",
        "    n_plus_one_gram_count = n_plusone_gram_counts[n_plus_one_gram] \\\n",
        "                    if n_plus_one_gram in n_plusone_gram_counts else 0\n",
        "    numerator = n_plus_one_gram_count + k\n",
        "\n",
        "    previous_n_gram = tuple(previous_n_gram)\n",
        "    previous_n_gram_count = n_gram_counts[previous_n_gram] \\\n",
        "                    if previous_n_gram in n_gram_counts else 0\n",
        "\n",
        "    denominator = previous_n_gram_count + k * vocabulary_size\n",
        "\n",
        "\n",
        "    probability = numerator / denominator\n",
        "    return probability"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZyKYDTkcRSD"
      },
      "source": [
        "### Evaluation of Language Model using Perplexity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDXsi2jycVdF"
      },
      "source": [
        "def perplexity(sentence,n_gram_counts, \n",
        "               n_plusone_gram_counts, \n",
        "               vocabulary, k=1.0, log_perplexity=True):\n",
        "    # find n as in n-grams\n",
        "    n = len(list(n_gram_counts.keys())[0])\n",
        "\n",
        "    sentence = [\"<s>\"] * n + sentence + [\"<e>\"]\n",
        "    sentence = tuple(sentence)\n",
        "\n",
        "    vocabulary_size = len(vocabulary)\n",
        "\n",
        "    # N = len(sentence) if n == 1 else len(sentence)-1\n",
        "    N = len(sentence)\n",
        "\n",
        "    if log_perplexity:\n",
        "        temp_sum = 0.0\n",
        "    else:\n",
        "        temp_product = 1.0\n",
        "\n",
        "    for t in range(n,N):\n",
        "        n_gram = sentence[t-n:t] \n",
        "        # Because the loop starts at n i.e. it excludes first n words as\n",
        "        # part of n-gram where t is current word so n-gram will be previous n\n",
        "        # words\n",
        "\n",
        "        word = sentence[t]\n",
        "\n",
        "        probability = calculate_probability(word, n_gram, \n",
        "                                            n_gram_counts,\n",
        "                                            n_plusone_gram_counts,\n",
        "                                            vocabulary, k=k)\n",
        "        if log_perplexity:\n",
        "            temp_sum += (1 / probability)\n",
        "        else:\n",
        "            temp_product *= (1 / probability)\n",
        "        \n",
        "    if log_perplexity:\n",
        "        perplexity = - (1/N) * temp_sum\n",
        "    else:\n",
        "        perplexity = temp_product**(1/N)\n",
        "\n",
        "    return perplexity"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qyMTYC1cVZd"
      },
      "source": [
        ""
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlOzAR-LcVXZ"
      },
      "source": [
        ""
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YiaR1ENNTtcf"
      },
      "source": [
        "def calculate_probabilities(previous_n_gram, \n",
        "                            n_gram_counts, n_plusone_gram_counts, \n",
        "                            vocabulary, k=1.0):\n",
        "    probabilities = dict()\n",
        "    vocabulary = vocabulary + [\"<e>\",\"<unk>\"]\n",
        "    vocabulary_size = len(vocabulary)\n",
        "\n",
        "    for word in vocabulary:\n",
        "        probability = calculate_probability(word,\n",
        "                                            previous_n_gram, \n",
        "                                            n_gram_counts, \n",
        "                                            n_plusone_gram_counts, \n",
        "                                            vocabulary_size,\n",
        "                                            k=1.0)\n",
        "        probabilities[word] = probability\n",
        "\n",
        "    return probabilities"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mesuwjy9cI7z"
      },
      "source": [
        "def suggest_a_word(previous_tokens, \n",
        "                   n_gram_counts, \n",
        "                   n_plusone_gram_counts, \n",
        "                   vocabulary, \n",
        "                   k=1.0,\n",
        "                   start_with=None):\n",
        "    \n",
        "    n = len(list(n_gram_counts.keys())[0])\n",
        "\n",
        "    previous_n_gram = previous_tokens[-n:] \n",
        "\n",
        "    probabilities = calculate_probabilities(previous_n_gram,\n",
        "                                            n_gram_counts,\n",
        "                                            n_plusone_gram_counts,\n",
        "                                            vocabulary,\n",
        "                                            k=k)\n",
        "    max_probability = 0\n",
        "    suggestion = None\n",
        "\n",
        "    for word, prob in probabilities.items():\n",
        "        if start_with:\n",
        "            if not word.startswith(start_with):\n",
        "                continue\n",
        "        \n",
        "        if prob > max_probability:\n",
        "            max_probability = prob\n",
        "            suggestion = word\n",
        "    return (suggestion, max_probability)"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcVbRBY9MhIB"
      },
      "source": [
        "# def get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0, start_with=None):\n",
        "#     model_counts = len(n_gram_counts_list) # 5\n",
        "#     suggestions = []\n",
        "#     for i in range(model_counts-1): #0 \n",
        "#         n_gram_counts = n_gram_counts_list[i] # uni\n",
        "#         n_plus1_gram_counts = n_gram_counts_list[i+1] # bi\n",
        "        \n",
        "#         suggestion = suggest_a_word(previous_tokens, n_gram_counts,\n",
        "#                                     n_plus1_gram_counts, vocabulary,\n",
        "#                                     k=k, start_with=start_with)\n",
        "#         suggestions.append(suggestion)\n",
        "    # return suggestions"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_AIbFPxNt-8"
      },
      "source": [
        "def get_suggestions(previous_tokens, \n",
        "                    n_gram_counts, \n",
        "                    n_plusone_gram_counts,\n",
        "                    vocabulary,\n",
        "                    k=1.0,\n",
        "                    n_suggestions=4,\n",
        "                    starts_with=None):\n",
        "    \n",
        "    n = len(list(n_gram_counts.keys())[0])\n",
        "\n",
        "    previous_n_gram = previous_tokens[-n:]\n",
        "\n",
        "    probabilities = calculate_probabilities(\n",
        "        previous_n_gram,\n",
        "        n_gram_counts,\n",
        "        n_plusone_gram_counts,\n",
        "        vocabulary,\n",
        "        k=k\n",
        "    )\n",
        "\n",
        "    sorted_probabilities = {\n",
        "        word : probability for word, probability in \\\n",
        "        sorted(probabilities.items(), key=lambda item: item[1], reverse=True)\n",
        "            }\n",
        "    if starts_with:\n",
        "        suggestions = dict()\n",
        "        for word, prob in sorted_probabilities.items():\n",
        "            if word.startswith(starts_with):\n",
        "                suggestions[word] = prob\n",
        "            else:\n",
        "                continue\n",
        "        return list(suggestions.items())[:n_suggestions]\n",
        "\n",
        "    return list(sorted_probabilities.items())[:n_suggestions]"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzUFML4RObPH",
        "outputId": "3403fca5-b618-41fd-afb0-7f9ba11b5273"
      },
      "source": [
        ""
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{3: 4, 4: 3}"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3dTHNZZMznS",
        "outputId": "84226e02-e70e-42d7-fc06-c9066f175e66"
      },
      "source": [
        "sentences = [['i', 'like', 'a', 'cat'],\n",
        "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
        "unique_words = list(set(sentences[0] + sentences[1]))\n",
        "\n",
        "unigram_counts = count_n_grams(sentences, 1)\n",
        "bigram_counts = count_n_grams(sentences, 2)\n",
        "\n",
        "previous_tokens = [\"i\", \"like\"]\n",
        "tmp_suggest1 = suggest_a_word(previous_tokens, unigram_counts, bigram_counts, unique_words, k=1.0)\n",
        "print(f\"The previous words are 'i like',\\n\\tand the suggested word is `{tmp_suggest1[0]}` with a probability of {tmp_suggest1[1]:.4f}\")\n",
        "\n",
        "print()\n",
        "# test your code when setting the starts_with\n",
        "tmp_starts_with = 'c'\n",
        "tmp_suggest2 = suggest_a_word(previous_tokens, unigram_counts, bigram_counts, unique_words, k=1.0, start_with=tmp_starts_with)\n",
        "print(f\"The previous words are 'i like', the suggestion must start with `{tmp_starts_with}`\\n\\tand the suggested word is `{tmp_suggest2[0]}` with a probability of {tmp_suggest2[1]:.4f}\")"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The previous words are 'i like',\n",
            "\tand the suggested word is `a` with a probability of 0.2727\n",
            "\n",
            "The previous words are 'i like', the suggestion must start with `c`\n",
            "\tand the suggested word is `cat` with a probability of 0.0909\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1RF88g1M543",
        "outputId": "100def65-5f62-491b-88e9-773e4fab594b"
      },
      "source": [
        "get_suggestions(previous_tokens,\n",
        "                unigram_counts,\n",
        "                bigram_counts,\n",
        "                unique_words,\n",
        "                k=1,\n",
        "                n_suggestions=4,\n",
        "                starts_with=\"c\"\n",
        "                )"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('cat', 0.09090909090909091)]"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9W0hc8ocRtT_"
      },
      "source": [
        "bigram_counts = count_n_grams(preprocessed_data, 2)\n",
        "trigram_counts = count_n_grams(preprocessed_data, 3)"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjA1eCk6TuNM"
      },
      "source": [
        "c"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1AU6GJCQUNOb",
        "outputId": "0bbe6add-5647-4d3b-f47f-c3ad97e5b4e9"
      },
      "source": [
        "get_suggestions(previous_tokens, bigram_counts, trigram_counts, vocabulary, k=1.0)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('the', 0.001925113100394648),\n",
              " ('to', 0.0014438348252959862),\n",
              " ('it', 0.0014438348252959862),\n",
              " ('that', 0.0013475791702762538)]"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnXFeoHGUSgQ",
        "outputId": "8ab3eac9-e7a7-4c07-8370-de48c567428b"
      },
      "source": [
        "previous_tokens = previous_tokens + ['the']\n",
        "get_suggestions(previous_tokens, unigram_counts, bigram_counts, unique_words, k=1.0)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('<unk>', 175.77777777777777),\n",
              " ('<e>', 4.666666666666667),\n",
              " ('dog', 1.5555555555555556),\n",
              " ('cat', 1.0)]"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMjhH71_UeeP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}